{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducrtion to Machine Learning: Assignment #2\n",
        "## Submission date: 19\\05\\2025, 23:59.\n",
        "### Topics:\n",
        "- Linear Regression\n",
        "- Perceptron\n",
        "- Logistic Regression\n",
        "- SVM"
      ],
      "metadata": {
        "id": "aj440z9b98Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by:\n",
        "\n",
        " **Student 1 Name+ID\n",
        "\n",
        " **Student 2 Name+ID"
      ],
      "metadata": {
        "id": "em4OeZTD9-R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instruction:**\n",
        "\n",
        "· Submissions in pairs only.\n",
        "\n",
        "· Try to keep the code as clean, concise, and short as possible\n",
        "\n",
        "· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n",
        "\n",
        "· <font color='red'>Please write your answers to question in red</font>.\n",
        "\n",
        "**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n",
        "\n",
        "**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"
      ],
      "metadata": {
        "id": "GQ-GUBWN9_JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Linear Regression\n",
        "You are requested to predict energy efficiency based on building attributes, such as wall area and glazing area.\n",
        "The dataset consists of 8 continious features and **Two** outputs - Heat & Cool loadings.\n",
        "You can read more about the dataset <a href='https://archive.ics.uci.edu/dataset/242/energy+efficiency'>here</a>"
      ],
      "metadata": {
        "id": "rs2flQjZL-D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "DhZGt3H7j4c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from https://sharon.srworkspace.com/ml/datasets/hw3/ENB2012_data.csv"
      ],
      "metadata": {
        "id": "J3qUuz18MR3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw3/ENB2012_data.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "RA8-1RrnMOpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the data into 85% train and 15% test with random_state=21.\n",
        "\n"
      ],
      "metadata": {
        "id": "4zjczKr4MNHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "FBnctOlsMVCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define sklearn's linear regression, fit the train and save both train and test MSEs (by appending to results['Linear']). <br/>\n",
        "DO NOT use \"score\" method of sklearn - we didn't learn about it and it does not measure MSE."
      ],
      "metadata": {
        "id": "11RHbDzUM1bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {'Linear': [], 'Ridge': [], 'Lasso': []}\n",
        "\n",
        "# Implement here"
      ],
      "metadata": {
        "id": "R4mxOKwiNOY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the correlation matrix of the train data. Should we do something?"
      ],
      "metadata": {
        "id": "_GGGh7IDPUec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "zjeP8fMxOUyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For both ridge and lasso, tune the best $\\lambda$.\n",
        "- For ridge, $\\lambda\\in \\text{np.arange}(0.1, 1, 0.1)$.\n",
        "- For lasso, $\\lambda\\in \\text{np.logspace}(-4, -2, 20)$\n",
        "\n",
        "For lasso, use `max_iter=5000`, as there is no close formula and convegence is not guranteeed."
      ],
      "metadata": {
        "id": "uyaVfDo2mINo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process data, if needed\n",
        "\n",
        "######### RIDGE #########\n",
        "\n",
        "alphas = np.arange(0.1, 1, 0.1)\n",
        "mses_train = []\n",
        "mses_test = []\n",
        "\n",
        "# implement here\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.plot(alphas, mses_train)\n",
        "plt.title(\"train mse (ridge)\")\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.plot(alphas, mses_test)\n",
        "plt.title(\"test mse (ridge)\")\n",
        "\n",
        "######### LASSO #########\n",
        "\n",
        "alphas = np.logspace(-4, -2, 20)\n",
        "mses_train = []\n",
        "mses_test = []\n",
        "\n",
        "# implement here\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.plot(alphas, mses_train)\n",
        "plt.title(\"train mse (lasso)\")\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.plot(alphas, mses_test)\n",
        "plt.title(\"test mse (lasso)\")"
      ],
      "metadata": {
        "id": "re_G7aE2mq0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the theory, both methods give best results for a very small $\\lambda$ and otherwise, big MSEs on both train and test. Look at the following:\n",
        "- the coefficients of the original regression model.\n",
        "- the mathematical relation between cov matrix and $X^\\top X$, and determinant of both.\n",
        "\n",
        "Use those to explain why larger values will only make things worse, even on the test.\n",
        "\n",
        "<font color='red'>Write your answer here and explain it</font>"
      ],
      "metadata": {
        "id": "H3mfhp-JnqBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print results from all three models (and use the best $\\lambda$ from each model).\n",
        "1. Explain the results and determine which model was be best.\n",
        "2. What might be the cause to the values for searching given above? Why are the differences in the scales?\n",
        "\n",
        "<font color='red'>Write your answers here and explain them</font>"
      ],
      "metadata": {
        "id": "xQ6qmbqWkq8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results, columns=['Linear', 'Ridge', 'Lasso'], index=['train', 'test'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "mrmH1VcanVU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 - Perceptron\n",
        "You are given dataset for binary classification in 2D and aim to build the best Perceptron classifier."
      ],
      "metadata": {
        "id": "NKWjqVSG-hZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "WcTtpyuLRnos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "S9McjsXS-inp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load npy file\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_npy_file(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n",
        "    return npy_data\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZMbzzvfpxAM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and make sure labels are appropriate for the perceptron algorithm"
      ],
      "metadata": {
        "id": "KhiHyOHbWCOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw2/perceptron_data.npy')\n",
        "\n",
        "X_train = data_dict['X_train']\n",
        "y_train = data_dict['y_train']\n",
        "X_test = data_dict['X_test']\n",
        "y_test = data_dict['y_test']\n",
        "\n",
        "# Change labels here if required. No scaling required"
      ],
      "metadata": {
        "id": "CnxdtJ9H-uMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function `perceptron(data, labels, batch_size)` which gets the train data $X\\in\\mathbb{R}^{n\\times d}$, labels and returns the weight vector learned by perceptron.\n",
        "\n",
        "At each iteration, sample `batch_size` miscalssified samples and update the weights according to them.\n",
        "\n",
        "You should return two values:\n",
        "1. List of the losses from all the iterations over all the data. For example, if the loss was 10 and in the next iteration 5, return [10,5].\n",
        "2. The weights w.\n",
        "\n",
        "Demands:\n",
        "- Do not use max_iterations bound here, but only gradient norm check.\n",
        "- Use lr=0.05."
      ],
      "metadata": {
        "id": "tQAuuHqiWjFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(data, labels, batch_size):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "o7VtxRETAD-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ploting function\n",
        "def plot(data, labels, w, bias):\n",
        "\n",
        "    plt.scatter(data[:,0], data[:,1], c=labels)\n",
        "\n",
        "    a, b, c = w[0], w[1], bias\n",
        "\n",
        "    m = -a / b\n",
        "    b = -c / b\n",
        "\n",
        "    x = np.arange(0.2, 0.8, 0.1)\n",
        "    y = m * x + b\n",
        "\n",
        "    plt.plot(x, y)\n",
        "\n",
        "    preds = np.sign(np.dot(data, w)+bias)\n",
        "    acc = np.count_nonzero(labels == preds) / len(labels)\n",
        "    plt.title(f\"Accuracy on data is {acc}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s4mWRarOv7Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the perceptron model and call the plot func' on the <u>train</u>\n",
        "dataset. It prints the accurcay as the graph's title.\n",
        "1. What is your conclusion about this data?\n",
        "2. How would SVM react to such data? Will it perform better?\n",
        "\n",
        "<font color='red'>Write here your answers and explain them.</font>"
      ],
      "metadata": {
        "id": "Ffu9HOcBv6dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, w = perceptron(X_train, y_train, batch_size=1)\n",
        "# call the plotting function here"
      ],
      "metadata": {
        "id": "AfQtjdHxCgUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the same training as above, but using batch_size=1,10,50,100.\n",
        "\n",
        "Plot all the graphs and compare them. In addition, what is your optimal batch_size for this problem? why?\n",
        "\n",
        "<font color='red'>Write here your answers and explain them.</font>"
      ],
      "metadata": {
        "id": "uSWn3A5jarJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "spRaCHdwY3jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the optimal batch_size you specified, print the final test accuracy."
      ],
      "metadata": {
        "id": "cnbaTIq9wOZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "tzt1OEPtwQD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 - Logistic regression\n",
        "\n",
        "In this section you will build a classifier on a \"toy\" problem - Based on two grades, we determine if student passes the course or not."
      ],
      "metadata": {
        "id": "PJPkQ__X2pKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries and load dataset"
      ],
      "metadata": {
        "id": "KCZXWQdQ4P_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw2/exams2.csv', header=None)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "011X9kCz4Prr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data:\n",
        "1. Convert dataset to numpy, make sure classes are {0,1}.\n",
        "2. The grades are discrete values between 0-100. Make them continious by adding a noise $ϵ_i\\sim\\mathcal{N}(0,1)$ for each data point.\n",
        "\n",
        "3. Split to temp & test (80-20, stratify, random_state=42)\n",
        "4. Split the temp to train & validation (80-20, stratify, random_state=42)\n",
        "5. Scale the data"
      ],
      "metadata": {
        "id": "U6BK3TZh4SFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Implement here"
      ],
      "metadata": {
        "id": "3XZQs2952rGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function `Logistic_Regression_via_GD(X, y, lr)`:\n",
        "-\tInput: the training data $X\\in\\mathbb{R}^{n\\times d}$, a label vector $y\\in\\{0,1\\}^n$ and learning rate parameter ‘lr’.\n",
        "-\tOutput: The function computes the output vector ‘w’ (and ‘b’) which minimzes the logistic regression cost function on ‘X’ and ‘y’. <br/>\n",
        "\n",
        "It should be done by implementing Gradient descent (with ‘lr’ as the learning rate) to solve logistic regression. Also, make sure to not run more than 3000 iterations. <br/>\n",
        "\n",
        "Tip: The gradients may be large, you can use $\\frac{1}{n}\\nabla{L}$ (which is the true empirical loss' gradient)"
      ],
      "metadata": {
        "id": "CBR_lRqe8N18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def Logistic_Regression_via_GD(X, y, lr):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "5hgUwWPF8QiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function 'predict' is already implemented for you. It gives a classification for a new sample (x) based on the sign of $w^\\top x + b$. <br/>\n",
        "The function currently returns {-1,+1}. <u>Modify this</u> to {0,1}"
      ],
      "metadata": {
        "id": "Zbue-ARwBVMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w,b,x):\n",
        "    return np.sign(np.dot(w, x) + b)"
      ],
      "metadata": {
        "id": "CqrZl4vfBvGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call `Logistic_Regression_via_GD(X,y,lr)`, where ‘X’ and ‘y’ are the training data and the corresponding labels. Use lr of 0.1 and print the accuracy on the test set."
      ],
      "metadata": {
        "id": "WG7pMuREAWO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w, b = # Implement here\n",
        "\n",
        "preds = # Implement here\n",
        "accuracy = # Implement here\n",
        "print(f\"Test accuracy is {accuracy * 100}%\")"
      ],
      "metadata": {
        "id": "6zL-7lp0AGcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the test data. As for now, it shows the scaled data.<br/>\n",
        "Show the data and line of the original data ranges (x,y ranges in [0,100])."
      ],
      "metadata": {
        "id": "CSTCGymL2OAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(data, labels, w, bias):\n",
        "\n",
        "    plt.scatter(data[:,0], data[:,1], c=labels)\n",
        "\n",
        "    a, b, c = w[0], w[1], bias\n",
        "\n",
        "    m = -a / b\n",
        "    b = -c / b\n",
        "\n",
        "    x = np.arange(np.min(data[:,0]), np.max(data[:,0]), 0.1)\n",
        "    y = m * x + b\n",
        "\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "\n",
        "plot(X_test, y_test, w, b)"
      ],
      "metadata": {
        "id": "S4ITLbDrm6R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to upgrade our model and add a regularization term. The loss will be\n",
        "\n",
        "$$\n",
        "L'=L+\\lambda ||w||_1\n",
        "$$\n",
        "\n",
        "Adjust the implementation of `Logistic_Regression_via_GD` accordingly and add a hyperparameter named 'lamda'.\n",
        "\n",
        "Plot a graph of the accuracy on the validation set as function of lamda. The range is up to your choice, but dont use a larger value than 2."
      ],
      "metadata": {
        "id": "QWCDmkWdSgOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "qa7H5IvKUQS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the best $\\lambda$ obtained from your work, print the final test accuracy.\n",
        "\n",
        "Answer the following questions:\n",
        "1. Did we need lambda? Try to justify why the lambda you found is the best.\n",
        "2. Generally speaking, when using the validation for tuning the best hyperparameter, is it guranteed it will be also the best for the test? Explain and show an example.\n",
        "\n",
        "<font color='red'>Write your answers here, with an explaination as requested</font>"
      ],
      "metadata": {
        "id": "RGixSilHUPXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 - SVM\n",
        "\n",
        "Answer those questions here (in color) or in pdf"
      ],
      "metadata": {
        "id": "6uCKMrAFuKNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First section\n",
        "\n",
        "We will change the algorithm for Hard SVM by learning only from the samples with positive labels and ignoring samples with negative labelings. Hence, the optimization problem becomes:\n",
        "\n",
        "$$\\min \\frac{1}{2}||w||^2\\\\\n",
        "\\text{s.t.} \\forall i\\in J_+, w^\\top x_i+b\\geq 1\n",
        "$$\n",
        "\n",
        "As $J_+$ is the set of positive labeled samples indexes.\n",
        "\n",
        "a. Under the settings above, what will be the solution of w? Justify.\n",
        "b. If we set b=0, meaning we remain only with $w^\\top x_i\\geq 1$, what is $\\min_{i\\in J_+} w^\\top x_i?$\n",
        "\n",
        "c. Let $w^*$ be the solution to the problem (b=0). We will classify new samples as:\n",
        "\n",
        "$$\n",
        "\\hat{y}=\\begin{cases}\n",
        "1 & w^\\top x\\geq \\min_{i\\in J^+} w^\\top x_i-ɛ \\\\\n",
        "-1 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "For some small $ε>0$. Will this condition classify correctly all the training samples, both positive and negative? Justify"
      ],
      "metadata": {
        "id": "c5mAOGz11hjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second section\n",
        "\n",
        "Consider three distinct points $x_1,x_2∈R^d$ with labels $y_1=1, y_2=-1$.\n",
        "Compute the hyperplane that Hard SVM will return on this data, i.e., give explicit expressions for w and b as functions of x1, x2.\n",
        "Hint: convert the primal problem to the dual one and reduce it to a one variable problem.\n"
      ],
      "metadata": {
        "id": "s8o6Km3M264O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third section\n",
        "\n",
        "We define the SHOR loss function of finding linear seperators:\n",
        "\n",
        "$$\n",
        "L=\\sum_{i=1}^n y_i*COST_0(\\theta^\\top x_i)+(1-y_i)*COST_1(\\theta^\\top x_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "<img src=\"https://srworkspace.com/sharon/ml/datasets/cost.png\"/>\n",
        "\n",
        "This loss will be zero if exactly two conditions hold true. Which two conditions gurantee this? Explain!\n",
        "\n",
        "(i) For every sample with y_i=1, we have that $\\theta^\\top x_i\\geq 0$\n",
        "\n",
        "(ii) For every sample with y_i=0, we have that $\\theta^\\top x_i\\leq 0$\n",
        "\n",
        "(iii) For every sample with y_i=1, we have that $\\theta^\\top x_i\\geq -1$\n",
        "\n",
        "(iv) For every sample with y_i=1, we have that $\\theta^\\top x_i\\geq 1$"
      ],
      "metadata": {
        "id": "GX32wZOK5xii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 - Bonus 8 pts"
      ],
      "metadata": {
        "id": "1UiJ5ncdC7R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the house prices dataset you have seen in class.\n",
        "In this question, you will use another loss.\n",
        "\n",
        "$$\n",
        "L=\\frac{1}{n}||Xw-y||_1=\\frac{1}{n}\\sum_{i=1}^n |w^\\top x_i-y_i|\n",
        "$$\n",
        "This loss is more robust to outliers, since outliers affect linearly and not squared. However, w that minimizes this loss has no closed formula.\n",
        "\n",
        "Implement here everything from scratch. Do not use an existing model to minimize the loss. Try to reach the best model you can\n",
        "\n",
        "**Report** in your results: train and test results, plots, tuning (if you choose to), including any evidence your solution is good enough <br/>\n",
        "<font color='red'>Report here about your choices and reasons (dealing with data, optimization methods)</font>\n"
      ],
      "metadata": {
        "id": "J-0oVe4ZDBUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/Toronto_rentals.csv')\n",
        "\n",
        "X = data.iloc[:, :3]  #Bedroom, bathroom, den\n",
        "y = data.iloc[:, -1].str.replace(r'[^\\d.]', '', regex=True).astype('float') #Price\n",
        "\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
      ],
      "metadata": {
        "id": "wsQDTBd7JA8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "98_8OkY1JHhy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rs2flQjZL-D7",
        "NKWjqVSG-hZG",
        "PJPkQ__X2pKK",
        "6uCKMrAFuKNp",
        "1UiJ5ncdC7R2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}