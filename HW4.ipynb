{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducrtion to Machine Learning: Assignment #4\n",
        "## Submission date: 15\\07\\2025, 23:59.\n",
        "### Topics:\n",
        "- AdaBoost\n",
        "- PAC, VCdim\n",
        "- Bias vs Variance\n",
        "- K means clustering\n",
        "- Decision Trees"
      ],
      "metadata": {
        "id": "iBHBci_i2IgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by:\n",
        "\n",
        " **Student 1 Name+ID\n",
        "\n",
        " **Student 2 Name+ID"
      ],
      "metadata": {
        "id": "NGo4MrnG2NXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instruction:**\n",
        "\n",
        "· Submissions in pairs only.\n",
        "\n",
        "· Try to keep the code as clean, concise, and short as possible\n",
        "\n",
        "· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n",
        "\n",
        "· <font color='red'>Please write your answers to question in red</font>.\n",
        "\n",
        "**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n",
        "\n",
        "**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"
      ],
      "metadata": {
        "id": "eietFcHy2Kr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load smiling dataset\n",
        "\n",
        "From now on, we will deal with the Smiling-face dataset, which determines if a person is smiling or not.\n",
        "\n",
        "You will try several models and hope to get good results<br/>\n",
        "Your task is: run the following section and make sure your understand what's going on."
      ],
      "metadata": {
        "id": "d_iKlHnjsiBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to your <a href=\"https://www.kaggle.com/\">Kaggle</a> account and under the settings, generate new API token. <br/>\n",
        "This will give you the json file, which you will upload here."
      ],
      "metadata": {
        "id": "ABkvwY8WrhPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The algorithm expects you to upload JSON file to it!\n",
        "\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "! kaggle datasets download chazzer/smiling-or-not-face-data\n",
        "! unzip -q smiling-or-not-face-data.zip -d data"
      ],
      "metadata": {
        "id": "XsC3aagYU8Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "lB0zaNLzslMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "V1j2in2zXhT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "process the images"
      ],
      "metadata": {
        "id": "1UTtYOlw9eDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proccess_data(folder):\n",
        "\timage_arrays = []\n",
        "\tfor filename in os.listdir(folder):\n",
        "\t\tfile_path = os.path.join(folder, filename)\n",
        "\t\timage = cv2.imread(file_path)\n",
        "\t\tgray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\t\timage_arrays.append(gray_image)\n",
        "\treturn np.array(image_arrays)\n",
        "\n",
        "smile = proccess_data('./data/smile')\n",
        "non_smile = proccess_data('./data/non_smile')\n",
        "\n",
        "dataset = np.vstack((non_smile, smile))\n",
        "dataset = dataset / 255\n",
        "\n",
        "labels = [0] * len(non_smile) + [1] * len(smile)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "hvQvtLOAXk8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "display smiling and non-smiling image."
      ],
      "metadata": {
        "id": "1nW9k369dhYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(121)\n",
        "plt.title(\"Smile\")\n",
        "plt.imshow(smile[0], cmap='gray')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Not smile\")\n",
        "plt.imshow(non_smile[0], cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QfbqqhCadGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print the smiling and non-smiling data + the united dataset along with labels."
      ],
      "metadata": {
        "id": "rEVo_5-k990Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'smile array size is (images, height, width)={smile.shape}')\n",
        "print(f'non smile array size is (images, height, width)={non_smile.shape}')\n",
        "print()\n",
        "print(f'dataset array size is {dataset.shape}')\n",
        "print(f'labels array size is {labels.shape}')"
      ],
      "metadata": {
        "id": "4hCqccgRZUzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepear train and test datasets, print their structure. Since you have to deal with 1d features, we flatten the squared image"
      ],
      "metadata": {
        "id": "nw3w7aGusyDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.2, stratify=labels, random_state=42)\n",
        "\n",
        "print(f'train size is {x_train.shape} and labels size is {y_train.shape}')\n",
        "print(f'test size is {x_test.shape} and labels size is {y_test.shape}')\n",
        "print()\n",
        "\n",
        "x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "x_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "print(f'flattened train size is {x_train_flatten.shape} ')\n",
        "print(f'flattened test size is {x_test_flatten.shape}')"
      ],
      "metadata": {
        "id": "dhyWZgWXbE4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Clustering\n",
        "\n",
        "We learned in the tutorials about partitional clustering and specifically – k means algorithm. <br/>\n",
        "In this question you will implement it and see some nice applications."
      ],
      "metadata": {
        "id": "wqPk-EK5tBJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "KTd61ral4Ju3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "NA919a0U4MFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the missing implementation of Kmeans. Since there are k clusters, we will label each point with {0,..,k-1}."
      ],
      "metadata": {
        "id": "7LiNstqG3peu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Kmeans:\n",
        "\n",
        "\tdef __init__(self, n_clusters, max_iter=100, random_state=123):\n",
        "\t\tself.n_clusters = n_clusters\n",
        "\t\tself.max_iter = max_iter\n",
        "\t\tself.random_state = random_state\n",
        "\n",
        "\tdef initialize_centroids(self, X):\n",
        "\t\tnp.random.RandomState(self.random_state)\n",
        "\t\trandom_idx = np.random.permutation(X.shape[0])\n",
        "\t\tcentroids = X[random_idx[:self.n_clusters]]\n",
        "\t\treturn centroids\n",
        "\n",
        "\tdef reassign_centroids(self, X, labels):\n",
        "\t\tcentroids = np.zeros((self.n_clusters, X.shape[1]))\n",
        "\t\t# Implement here\n",
        "\t\treturn centroids\n",
        "\n",
        "\tdef compute_distance(self, X, centroids):\n",
        "\t\tdistance = np.zeros((X.shape[0], self.n_clusters))\n",
        "\t\tfor k in range(self.n_clusters):\n",
        "\t\t\trow_norm = np.linalg.norm(X - centroids[k, :], axis=1)\n",
        "\t\t\tdistance[:, k] = np.square(row_norm)\n",
        "\t\treturn distance\n",
        "\n",
        "\tdef find_closest_cluster(self, distance):\n",
        "\t\treturn np.argmin(distance, axis=1)\n",
        "\n",
        "\tdef compute_sse(self, X, labels, centroids):\n",
        "\t\tdistance = np.zeros(X.shape[0])\n",
        "\t\tfor k in range(self.n_clusters):\n",
        "\t\t\tdistance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=1)\n",
        "\t\treturn np.sum(np.square(distance))\n",
        "\n",
        "\tdef fit(self, X):\n",
        "\t\tself.centroids = self.initialize_centroids(X)\n",
        "\t\tfor i in range(self.max_iter):\n",
        "\t\t\told_centroids = self.centroids\n",
        "\t\t\t# For each point, calculate distance to all k clustes.\n",
        "\t\t\tself.labels =\t# Assign the labels with closest distance' cluster.\n",
        "\t\t\tself.centroids = # Update the centroids\n",
        "\t\t\tif np.all(old_centroids == self.centroids):\n",
        "\t\t\t\tbreak\n",
        "\t\tself.error = self.compute_sse(X, self.labels, self.centroids)\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tdistance = self.compute_distance(X, self.centroids)\n",
        "\t\treturn self.find_closest_cluster(distance)"
      ],
      "metadata": {
        "id": "Y8ACpogUs4ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test this on toy example"
      ],
      "metadata": {
        "id": "Wq9V9FPIwtjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw4/exams.csv', header=None).to_numpy()\n",
        "data, labels = db[:,:-1], db[:,-1]\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GVLM73H9vb69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to divide the data into 2 clusters. <br/>\n",
        "Define Kmeans object and fit the data."
      ],
      "metadata": {
        "id": "i3yHa5ap6k3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clust = Kmeans(n_clusters=2)\n",
        "clust.fit(data)\n",
        "\n",
        "# This code plots the clustered data with centroids\n",
        "labels = clust.labels\n",
        "centroids = clust.centroids\n",
        "\n",
        "c0 = data[labels == 0]\n",
        "c1 = data[labels == 1]\n",
        "\n",
        "plt.scatter(c0[:,0], c0[:,1], c='green', label='cluster 1')\n",
        "plt.scatter(c1[:,0], c1[:,1], c='blue', label='cluster 2')\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='black', label='centroid')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DV8XrX6AvdH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, apply clustering from the smiling faces dataset.\n",
        "\n",
        "Use the Elbow Method to choose another number of centroids between 1-5. <br/>\n",
        "<font color='red'>Explain your choice</font>"
      ],
      "metadata": {
        "id": "VwiX1POFx9TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sse = []\n",
        "list_k = list(range(1, 6))\n",
        "\n",
        "for k in list_k:\n",
        "  sse.append(error_of_current_clustering)\n",
        "\n",
        "'''Plot sse against k'''\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(list_k, sse, '-o')\n",
        "plt.xlabel(r'Number of clusters *k*')\n",
        "plt.ylabel('Sum of squared distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-3ebxS6y87C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply clustering with the selected k"
      ],
      "metadata": {
        "id": "NbZZOx8W8BTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "xR-GMy467BdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 - Adaboost\n",
        "\n",
        "In this exercise you will (pratially) implement AdaBoost and see how boosting can be applied to real-world problems, in detection of smiling or non-smiling face. <br/>\n",
        "\n"
      ],
      "metadata": {
        "id": "c31dFqWmuW-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, we saw AdaBoost only on 2d data and chose an axis aligned threshold, in parallel x or y axis, since we had 2 features.\n",
        "\n",
        "In d dimensions we will still choose an axis aligned threshold (hyperplane) but along the feature axis (1,…,d) that gives the best weak learner."
      ],
      "metadata": {
        "id": "8QyaKfT0Dd9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Find the best classifier. You can see this as blackbox\n",
        "# Those who want to understand more - talk to me :)\n",
        "\n",
        "def WL_by_sign_b(X_train, y_train, D, sign_b):\n",
        "    F_min = sum(D[y_train == sign_b])       #the max error weight is all those who match sign_b\n",
        "    tetha_min = X_train[0,0] - 1\n",
        "    J_min = 0\n",
        "    m, d = X_train.shape\n",
        "\n",
        "    for j in range(d):    #choose index j for h, d = 5000\n",
        "        values_wrt_j = np.column_stack((X_train[:,j],y_train, D))\n",
        "        values_wrt_j = values_wrt_j[np.argsort(values_wrt_j[:, 0])]\n",
        "        last = np.array([values_wrt_j[-1,0]+1, 0, 0])\n",
        "        values_wrt_j = np.vstack((values_wrt_j, last))     # add x_m+1,j = x_m,y  +1\n",
        "        F = sum(D[y_train == sign_b])\n",
        "\n",
        "        #choose tetha for h\n",
        "        Fs = F - np.cumsum(sign_b * values_wrt_j[:m,1] * values_wrt_j[:m,2])\n",
        "        min_idx = np.argmin(Fs)\n",
        "        if Fs[min_idx] < F_min:\n",
        "          F_min = Fs[min_idx]\n",
        "          tetha_min = 0.5 * sum(values_wrt_j[min_idx:min_idx+2,0])\n",
        "          J_min = j\n",
        "\n",
        "    return (J_min,tetha_min, F_min)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "48-M-cJ9uW-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given the function ```get_hypothesis(X_train, y_train, D)``` which uses the train data, labels and the distribution over the samples (weights) and returns the best classifier.\n",
        "\n",
        "Make sure to understand this tuple and the meaning of its variables."
      ],
      "metadata": {
        "id": "bm52HdIuEMgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hypothesis(X_train, y_train, D):\n",
        "    \"\"\"\n",
        "    return best weak learner h\n",
        "    h = (h_pred, h_index, h_theta)\n",
        "    - h_index is the feature in which the axis-aligned hyperplane is\n",
        "    - h_theta is the treshold\n",
        "    - h_pred is the label assigned if the feature h_index of the sample x <= h_theta\n",
        "    \"\"\"\n",
        "\n",
        "    Min_feature_idx1, tetha_min1, F_min1 = WL_by_sign_b(X_train, y_train, D, 1)\n",
        "    Min_feature_idx2, tetha_min2, F_min2 = WL_by_sign_b(X_train, y_train, D, -1)\n",
        "    if F_min1 < F_min2:\n",
        "        return (1, Min_feature_idx1, tetha_min1)\n",
        "    else:\n",
        "        return (-1, Min_feature_idx2, tetha_min2)\n",
        "\n",
        "# returns +1 | -1 only. For 0 returns +1. Use this\n",
        "def sign(expr):\n",
        "  return np.sign(np.sign(expr) + 0.5)"
      ],
      "metadata": {
        "id": "v12AG-XLEM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the following:<br/>\n",
        "- ```predict_wrt_h(X, h)```: should return the predictions of $X\\in\\mathbb{R}^{n\\times d}$, according to the classifier.\n",
        "- ```zo_loss_wrt_h(X, y, h)```: returns the 0-1 loss (delta) for $X\\in\\mathbb{R}^{n\\times d}$, if predictions differ from the real labels.\n",
        "- ```get_weighted_error(X_train, y_train, D, h)```: returns the $ε_t$, the weights sum of the misclassified samples.\n"
      ],
      "metadata": {
        "id": "PYGzm2sVEcs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_wrt_h(X, h):\n",
        "    \"\"\"\n",
        "    h = (h_pred, h_index, h_theta)\n",
        "    - h_index is the feature in which the axis-aligned hyperplane is\n",
        "    - h_theta is the treshold\n",
        "    - h_pred is the label assigned if the feature h_index of the sample x <= h_theta\n",
        "\n",
        "    This function should return h_pred if the h_index feature of X is less (or equal) than the treshold h_theta\n",
        "    otherwise, returns the opposite of h_pred\n",
        "    \"\"\"\n",
        "    # Implement here\n",
        "\n",
        "def zo_loss_wrt_h(X, y, h):\n",
        "    \"\"\"\n",
        "    return delta(h(x),y), which is array of 0-1\n",
        "    \"\"\"\n",
        "    # Implement here\n",
        "\n",
        "def get_weighted_error(X_train, y_train, D, h):\n",
        "    \"\"\"\n",
        "    return epsilon_t\n",
        "    e.g sum of D[i]*delta(h(xi),yi)\n",
        "    \"\"\"\n",
        "    # Implement here"
      ],
      "metadata": {
        "id": "l3LlqfbxuW-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement ```run_adaboost(X_train, y_train, T)```: runs the AdaBoost algorithm for T steps and returns a list of classifiers (hypotheses), and list of alpha values ($α_t$ for each iteration T). <br/>\n",
        "Hint: the only 'for' loop needed is over T. The rest can be done efficiently using numpy and previous implemented functions."
      ],
      "metadata": {
        "id": "nGpM7RZVE8kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_adaboost(X_train, y_train, T):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        hypotheses :\n",
        "            A list of T tuples describing the hypotheses chosen by the algorithm.\n",
        "            Each tuple has 3 elements (h_pred, h_index, h_theta)\n",
        "            h = (h_pred, h_index, h_theta)\n",
        "            - h_index is the feature in which the axis-aligned hyperplane is\n",
        "            - h_theta is the treshold\n",
        "            - h_pred is the label assigned if the feature h_index of the sample x <= h_theta\n",
        "        alpha_vals :\n",
        "            A list of T float values, which are the alpha values obtained in every\n",
        "            iteration of the algorithm.\n",
        "    \"\"\"\n",
        "    alpha_vals = []\n",
        "    hypotheses = []\n",
        "    D = np.ones(len(X_train)) / len(X_train)\n",
        "\n",
        "    #Implement here\n",
        "\n",
        "    return hypotheses, alpha_vals"
      ],
      "metadata": {
        "id": "NOuxZoKVuW-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement ```calc_error(data, labels, hypotheses, alpha_vals)```:\n",
        "\n",
        "For each iteration $t=1,...,T$, calculate the error rate (from $1,...,t$). <br/>"
      ],
      "metadata": {
        "id": "pSlZVoK3FRlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_error(data, labels, hypotheses, alpha_vals):\n",
        "    \"\"\"\n",
        "    calc empirical error\n",
        "    classfication is sign of sum of a_t*h_t(x)\n",
        "\n",
        "    Return list of mean errors using 1,...,T hypotheses\n",
        "    \"\"\"\n",
        "    # Implement here\n",
        "\n",
        "    return error_all_t"
      ],
      "metadata": {
        "id": "o4f97FFluW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepear the adaboost run.\n",
        "- 80 iterations.\n",
        "- Use the labels 0/1 into -1/1.\n",
        "\n",
        "This is due to the fact that classification based on sign which is -1,+1."
      ],
      "metadata": {
        "id": "0PY_iB4JFd6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 80\n",
        "\n",
        "y_train_ada = y_train.copy()\n",
        "y_test_ada = y_test.copy()\n",
        "\n",
        "y_train_ada[y_train == 0] = -1\n",
        "y_test_ada[y_test == 0] = -1"
      ],
      "metadata": {
        "id": "JPe5wLOJuW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the AdaBoost algorithm on the train and test <b>after</b> performing PCA."
      ],
      "metadata": {
        "id": "Uniy817jFnj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses, alpha_vals = run_adaboost(x_train_new, y_train_ada, T)"
      ],
      "metadata": {
        "id": "Ry-KlBZ7uW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the train and test erros as function of T"
      ],
      "metadata": {
        "id": "4W6ZAfdpFukB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_error_t = calc_error(x_train_new, y_train_ada, hypotheses, alpha_vals)\n",
        "test_error_t = calc_error(x_test_new, y_test_ada, hypotheses, alpha_vals)\n",
        "X = np.arange(1,T+1)\n",
        "plt.plot(X, train_error_t ,'b+', label = 'error of train set')\n",
        "plt.plot(X, test_error_t ,'ro', label = 'error of test set')\n",
        "plt.xlabel('t')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r6LlAD8buW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the train and test accuracy"
      ],
      "metadata": {
        "id": "PWE3XKCUFyTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'final train accuracy {1-train_error_t[-1]}')\n",
        "print(f'final test accuracy {1-test_error_t[-1]}')"
      ],
      "metadata": {
        "id": "5G3aoRcruW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the AdaBoost algorithm on the train and test <b>before</b> performing PCA. <br/>\n",
        "\n",
        "Note: an efficient implementation should take up to 4 minutes of running."
      ],
      "metadata": {
        "id": "nnrh0s37uW-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses, alpha_vals = run_adaboost(x_train_flatten, y_train_ada, T)\n",
        "train_error_t = calc_error(x_train_flatten, y_train_ada, hypotheses, alpha_vals)\n",
        "test_error_t = calc_error(x_test_flatten, y_test_ada, hypotheses, alpha_vals)\n",
        "X = np.arange(1,T+1)\n",
        "plt.plot(X, train_error_t ,'b+', label = 'error of train set')\n",
        "plt.plot(X, test_error_t ,'ro', label = 'error of test set')\n",
        "plt.xlabel('t')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dDaS3jeuW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'final train accuracy {1-train_error_t[-1]}')\n",
        "print(f'final test accuracy {1-test_error_t[-1]}')"
      ],
      "metadata": {
        "id": "y5Y0WXSouW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following questions:\n",
        "- Which dataset would you prefer to run on AdaBoost - before or after PCA?\n",
        "- What are your conclusions on using AdaBoost in general?\n",
        "\n",
        "<font color='red'>Write here your answers and explain them</font>"
      ],
      "metadata": {
        "id": "L2Kl1lAFGmBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 - Decision trees\n",
        "\n",
        "You are requested by the Faculty of Biology to construct a classifier for predicting the quality of different fruits by several features.\n",
        "<br/> For this, you will use decision trees!\n"
      ],
      "metadata": {
        "id": "loHiZy0lVzT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 7 features which were obtained from the biologists that collected data and normalized it to you.<br/>\n",
        "\n",
        "Note that the features are continuous! Therefore, use only one threshold and split according to it."
      ],
      "metadata": {
        "id": "34peWE4XiSmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "w8LUgNO8V1Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data, print the first rows\n",
        "\n",
        "data = pd.read_csv('https://srworkspace.com/sharon/ml/datasets/hw3/banana_quality.csv')\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "-PZdW625WCTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete this missing implementation of the following functions:\n",
        "- ```calculate_entropy(self, data)``` – given data, compute the entropy, where the distribution is over its labels (target class).\n",
        "- ```calculate_information_gain(self, data, feature)``` – given data and specific feature, compute the information gain given by selecting that feature.\n",
        "\n",
        "Algorithm: The data is continuous, so create 15 thresholds between the min and max values of that feature. For each threshold, split to left tree and right tree and calculate the gain. Choose the threshold which gives the highest gain, along with the gain itself (to later compare between features) <br/>\n",
        "\n",
        "Tip: To split the tree (represented by data df), use filter_data.\n",
        "For example, when calculating the gain of 'skew' with threshold 0.5, you can create the left tree by use ```filter_data(data, 'skew', '0.5', left=True)``` to obtain only those samples."
      ],
      "metadata": {
        "id": "aISL11PniuiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ID3 decision tree class\n",
        "class DecisionTree:\n",
        "\tdef __init__(self):\n",
        "\t\tself.tree = {}\n",
        "\n",
        "\tdef calculate_entropy(self, data):\n",
        "\t\tlabels = data.iloc[:, -1]\n",
        "\t\t# Implement here\n",
        "\n",
        "\tdef calculate_information_gain(self, data, feature):\n",
        "\t\ttotal_entropy = self.calculate_entropy(data)\n",
        "\t\tinformation_gain = total_entropy\n",
        "\n",
        "\t\tvalues = # generate 15 thresholds\n",
        "\t\tbest_treshold = None\n",
        "\t\tbest_gain = 0\n",
        "\t\tfor value in values:\n",
        "\t\t\t# Implement here\n",
        "\n",
        "\t\treturn best_gain, best_treshold\n",
        "\n",
        "\tdef filter_data(self, data, feature, value, left=True):\n",
        "\t\tif left:\n",
        "\t\t\treturn data[data[feature] <= value].drop(feature, axis=1)\n",
        "\t\telse:\n",
        "\t\t\treturn data[data[feature] > value].drop(feature, axis=1)\n",
        "\n",
        "\tdef create_tree(self, data, depth=0):\n",
        "\t\t# Recursive function to create the decision tree\n",
        "\t\tlabels = data.iloc[:, -1]\n",
        "\n",
        "\t\t# Base case: if all labels are the same, return the label\n",
        "\t\tif len(np.unique(labels)) == 1:\n",
        "\t\t\treturn list(labels)[0]\n",
        "\n",
        "\t\tfeatures = data.columns.tolist()[:-1]\n",
        "\t\t# Base case: if there are no features left to split on, return the majority label\n",
        "\t\tif len(features) == 0:\n",
        "\t\t\tunique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\t\t\tmajority_label = unique_labels[label_counts.argmax()]\n",
        "\t\t\treturn majority_label\n",
        "\n",
        "\t\tselected_feature = None\n",
        "\t\tbest_gain = 0\n",
        "\t\tbest_treshold = None\n",
        "\n",
        "\t\tfor feature in features:\n",
        "\t\t\tgain, treshold = self.calculate_information_gain(data, feature)\n",
        "\t\t\tif gain >= best_gain:\n",
        "\t\t\t\tselected_feature = feature\n",
        "\t\t\t\tbest_treshold = treshold\n",
        "\t\t\t\tbest_gain = gain\n",
        "\n",
        "\t\t# Create the tree node\n",
        "\t\ttree_node = {}\n",
        "\t\ttree_node[(selected_feature, f\"<={best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=True), depth+1)\n",
        "\t\ttree_node[(selected_feature, f\">{best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=False), depth+1)\n",
        "\n",
        "\t\t# check if can unite them.\n",
        "\t\tif not isinstance(tree_node[(selected_feature, f\"<={best_treshold}\")], dict) and \\\n",
        "\t\t\t\tnot isinstance(tree_node[(selected_feature, f\">{best_treshold}\")], dict):\n",
        "\t\t\tif tree_node[(selected_feature, f\"<={best_treshold}\")] == tree_node[(selected_feature, f\">{best_treshold}\")]:\n",
        "\t\t\t\treturn tree_node[(selected_feature, f\"<={best_treshold}\")]\n",
        "\n",
        "\t\treturn tree_node\n",
        "\n",
        "\tdef fit(self, data):\n",
        "\t\tself.tree = self.create_tree(data)\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tX = [row[1] for row in X.iterrows()]\n",
        "\n",
        "\t\t# Predict the labels for new data points\n",
        "\t\tpredictions = []\n",
        "\n",
        "\t\tfor row in X:\n",
        "\t\t\tcurrent_node = self.tree\n",
        "\t\t\twhile isinstance(current_node, dict):\n",
        "\t\t\t\tsplit_condition = next(iter(current_node))\n",
        "\t\t\t\tfeature, value = split_condition\n",
        "\t\t\t\ttreshold = float(value[2:])\n",
        "\t\t\t\tif row[feature] <= treshold:\n",
        "\t\t\t\t\tcurrent_node = current_node[feature, f\"<={treshold}\"]\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcurrent_node = current_node[feature, f\">{treshold}\"]\n",
        "\t\t\tpredictions.append(current_node)\n",
        "\n",
        "\t\treturn predictions\n",
        "\n",
        "\tdef _plot(self, tree, indent):\n",
        "\t\tdepth = 1\n",
        "\t\tfor key, value in tree.items():\n",
        "\t\t\tif isinstance(value, dict):\n",
        "\t\t\t\tprint(\" \" * indent + str(key) + \":\")\n",
        "\t\t\t\tdepth = max(depth, 1 + self._plot(value, indent + 2))\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\" \" * indent + str(key) + \": \" + str(value))\n",
        "\t\treturn depth\n",
        "\n",
        "\tdef plot(self):\n",
        "\t\tdepth = self._plot(self.tree, 0)\n",
        "\t\tprint(f'depth is {depth}')\n"
      ],
      "metadata": {
        "id": "-IqUorHMV-tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are ready - define DecisionTree, fit it on the entire data and plot the tree. <br/>\n",
        "The depth of the tree should be 7"
      ],
      "metadata": {
        "id": "IdezpzK9WHwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "HgSriNidWHW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree is pretty large (depth is 7). To solve this, lets modify our DecisionTree: <br/>\n",
        "```\n",
        "def __init__(self, max_depth=np.inf):\n",
        "        self.tree = {}\n",
        "        self.max_depth = max_depth\n",
        "```\n",
        "\n",
        "Modify the rest of the code to stop growing after max_depth. <br/>\n",
        "Hint: When reached to max_depth, should we continue splitting? Which category will be best to be selected?\n"
      ],
      "metadata": {
        "id": "RL_ISCptkHCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare max_depth=5 to max_depth=7. Based on the results, which depth is better for our problem in terms of ML? <br/>\n",
        "<font color='red'>Write your answer here and explain it</font>"
      ],
      "metadata": {
        "id": "jlbxf36OWuEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "for depth in [5, 7]:\n",
        "  print(f\"------------ max_depth={depth} \"------------\")\n",
        "  # Implement here\n",
        "  print(f'Training accuracy is {acc}')\n",
        "\n",
        "  # Implement here\n",
        "  print(f'Test accuracy is {acc}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "yCV8Ia24WwBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use KFold (as seen in tutorials) for a cross validation search to the best depth for the tree."
      ],
      "metadata": {
        "id": "E14fEnmQwi__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accs = []\n",
        "\n",
        "for depth in tqdm(range(1,8)):\n",
        "    # Implement here\n",
        "\n",
        "plt.plot(range(1,6), accs)\n",
        "plt.xticks(range(1,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SjBmMq6Dwuwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use that depth and print the test score. Is it better generalizer than the first one?\n",
        "\n",
        "What do you conclude about the tuning proccess using validation? (answer to yourselves)."
      ],
      "metadata": {
        "id": "KLHtD2ElyTOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "vrsWjNPtg8Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 - PAC, VCdim\n",
        "\n",
        "Answer here or in the pdf, with full explanations."
      ],
      "metadata": {
        "id": "FnLxwkND0Cm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 1:\n",
        "\n",
        "To review, let the sample space be $X = [0, 1]$ and assume we study a binary classification problem, i.e. $Y = \\{0, 1\\}$. We will try to learn using an hypothesis class that consists of $k$ intervals.\n",
        "\n",
        "More explicitly, let $I = \\{[l1, u1], . . . , [lk, uk]\\}$ be $k$ disjoint\n",
        "intervals, such that $0 ≤ l_1 ≤ u_1 ≤ l_2 ≤ u_2 ≤ . . . ≤ u_k ≤ 1$. For each such $k$ disjoint intervals, define the corresponding hypothesis as\n",
        "$$\n",
        "h_I(x)=\\begin{cases}1 & x\\in[l_1, u_1] \\cup\\ldots\\cup [l_k, u_k] \\\\ 0 & \\text{else}\\end{cases}\n",
        "$$\n",
        "\n",
        "Finally, define $\\mathcal{H}$ as the hypothesis class that consists of all hypotheses that correspond to $k$ disjoint intervals:\n",
        "$$\n",
        "\\mathcal{H}_k=\\{h_I|I=\\{[l_1, u_1], . . . , [l_k, u_k]\\}, 0 ≤ l_1 ≤ u_1 ≤ l_2 ≤ u_2 ≤ . . . ≤ u_k ≤ 1\\}\n",
        "$$\n",
        "\n",
        "Moreover, the data will be generated from the probability:\n",
        "$$\n",
        "\\mathbb{P}(Y=1|x)=\\begin{cases}0.8 & x\\in[0,0.2]\\cup[0.4,0.6]\\cup[0.8,1]\\\\0.1 & x\\in(0.2,0.4)\\cup(0.6,0.8)\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "CJIpgms62_t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following questions:\n",
        "1. Is this PAC or Agnostic PAC model? Explain.\n",
        "2. How the optimal classifier $h\\in\\mathcal{H}$ will look like?\n",
        "3. What is the real-world error of that $h$?\n"
      ],
      "metadata": {
        "id": "ahyDaZy13CGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 2:\n",
        "\n",
        "A circle (r,c) is defined by its center c and its radius r. Look at the following classifiers family:\n",
        "$$\n",
        "H=\\{h_{(r,c)}:  r\\in\\mathbb{R},c∈\\mathbb{R}^2 \\}$$\n",
        "\n",
        "where $h_{(r,c)}(x)=1$ iff $x$ inside the circle $(r,c)$\n",
        "\n",
        "Find the VCdim of this class with full proof."
      ],
      "metadata": {
        "id": "eabNBYqKGm2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: 15 pts\n",
        "Build a model which gets at least 90% accuracy on the test of the smiling faces data. Please Submit only your final model with results.\n",
        "\n",
        "**Do not** try to use techniques we didn't learn about in the course.\n",
        "\n",
        "<font color='red'>Explain your choices and what you have tried</font>"
      ],
      "metadata": {
        "id": "Pp7PGabJtENB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "go1XwoObzFgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d_iKlHnjsiBj",
        "wqPk-EK5tBJT",
        "c31dFqWmuW-w",
        "loHiZy0lVzT0",
        "FnLxwkND0Cm1",
        "Pp7PGabJtENB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}